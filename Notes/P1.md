
***使用一个page的流程***

```c++
page_id_t page_id_temp;
auto *page0 = bpm->NewPage(&page_id_temp);
```

`BufferPoolManager::NewPage(page_id_t *page_id) -> Page*`是我们需要实现的一个函数, 先想象一下我们需要为分配一个*page*记录什么:

1. 我们应该有维护`page_id -> frame_id`的表, 在类中应该是`std::unordered_map<page_id_t, frame_id_t> page_table_`
2. 这里将*page*的句柄`Page*`控制权交给了调用者, *BPM*应该也有维护*page*的元信息, 在类中是`Page *pages_`
3. 创建新的*page*然后放在空闲的*frame*中, 对应的数据结构是`std::list<frame_id_t> free_list_`, 构造函数中能看到这个空闲链表大小是`pool_size`, 所以分配*page*


### 锁

***page_table_:***

1. `NewPage()`         ---> `RecoverPageInPool()`
2. `FetchPage()`  只读 ---> `RecoverPageInPool()`  
3. `UnpinPage()`  只读
4. `FlushPage()`  只读
5. `DeletePage()` 只读 ---> `RecoverPageInPool()` 
6. `RecoverPageInPool()` 只写

***free_list:***

1. `NewPage()`   ---> `Try2GetFid()`
2. `FetchPage()` ---> `Try2GetFid()`  
3. `DeletePage()` 只写
4. `Try2GetFid()` 先读后写

***page***

1. `FlushPage()`  只写
2. `RecoverPageInPool()` 只写
3. `NewPage()` 只读 ---> `FlushPage()`, `RecoverPageInPool()`
4. `UnpinPage` 只写


### 优化调度

1. 更好的替换算法。鉴于获取的工作量是倾斜的（即某些页面的访问频率高于其他页面），可以在设计`LRU-k`替换器时将页面访问类型考虑在内，以减少页面缺失。

2. 并行 I/O 操作。你可以同时向磁盘管理器发出多个请求，而不是在磁盘调度程序中一次处理一个请求。这种优化在现代存储设备中非常有用，并发访问磁盘可以更好地利用磁盘带宽。你应该处理队列中存在对同一页面的多个操作的情况，这些请求的最终结果应该是按顺序处理的。在单线程中，它们应具有读写后一致性。 (**不能只简单开多个后台处理线程, 从同一个请求队列取, 即不允许同时处理对一个page的读和写**)

3. 要在磁盘调度程序中实现真正的并行性，还需要允许缓冲池管理器处理多个`ReadPage`和`WritePage`请求，并同时驱逐多个页面。你可能需要在缓冲池管理器中引入一个条件变量来管理空闲页面。

4. 您可以使用我们在`third_party/readerwriterqueue`中提供的无锁队列，并创建自己的与`std::promise`兼容的`promise`，以降低线程间通信的开销。请注意，在本项目中，所有请求都必须通过 `DiskScheduler`的后台线程。

测试命令行:

> ./test/buffer_pool_manager_test; ./test/disk_scheduler_test; ./test/lru_k_replacer_test; ./test/page_guard_test

> make buffer_pool_manager_test -j `nproc`
> make disk_scheduler_test -j `nproc`
> make lru_k_replacer_test -j `nproc`
> make page_guard_test -j `nproc`

> make buffer_pool_manager_test -j `nproc` & make disk_scheduler_test -j `nproc` & make lru_k_replacer_test -j `nproc` & make page_guard_test -j `nproc` &


> cmake .. -DCMAKE_BUILD_TYPE=RelWithDebInfo
> make -j `nproc` bpm-bench
> ./bin/bustub-bpm-bench --duration 5000 --latency 1

不完善的并行I/O结果(存在数据不一致)

```sh
# enable_latency = 1
# scan_thread_n = 8
# get_thread_n = 8
# bustub_page_cnt = 6400
# bustub_bpm_size = 64
# lru_k_size = 16
256
scan: 6941.035378772736
get: 5481.1113332000805

512
scan: 7110.933439936038
get: 5460.723565860483

1024  
scan: 7699.840127897682
get: 5557.75379696243

```

# 改

## LRU-K 

这个比较容易, 我开了3个节点, `head->...->k_mid->...->tail`, 新的*frame_id*从队尾加入, 每次需要记录访问的时候就让*accses_k_*自增即可, 仍然小于规定的*k_*的话就什么都不要动, 保持**FIFO**的顺序, 而如果大于*k_*的话, 就从队头重新加入;

驱逐的时候先扫描`k_mid->...->tail`, 再扫描`k_mid->...->head`即可;

## 在线测试

这里是过不了的部分

***Page部分***

1. BufferPoolManagerTest.PagePinHardTest (0/4)
2. BufferPoolManagerTest.DeletePageTest (0/4)
3. BufferPoolManagerTest.RandomPagesTest (0/4)

***并发测试***

1. BufferPoolManagerTest.ConcurrentReaderWriterTest (0/6)
2. BufferPoolManagerTest.ConcurrentWriterTest (0/5)

***bpm***

不能稳定过都不算过(怎么会有0延迟刷盘啊 /哭 我之前)

1. Leaderboard.QPS.1 (0/4) `./bin/bustub-bpm-bench --duration 30000 --latency 0`
2. Leaderboard.QPS.2 (0/4) `./bin/bustub-bpm-bench --duration 30000 --latency 0 --lru-k-size 64 --db-size 640`

第三个测试用来检验一下性能(不开多线程IO真的很慢, 开了会数据不一致)
`./bin/bustub-bpm-bench --duration 30000 --latency 1`

我靠, 这怎么改, 痛定思痛, 我决定全部换回大锁再说, 太痛苦了.

## 预备工作

之前学习6.5840(分布式)的时候, 有一定的痛苦经历, 所以深知打印日志信息的重要性, 这里先规范一下输出.

用一个小一点的bpm测试吧: `./bin/bustub-bpm-bench --duration 30000 --latency 0 --lru-k-size 64 --db-size 640`

前面我看到过有`LOG_DEBUG("....");`用来打印日志信息的, 查看发现是调用`logger.h`中的`OutputLogHeader()`, 不过显示的时间粒度很大, 所以我这里改成秒+毫秒级别的信息, 使用`<chrono>`

```c++
#include <chrono> // 添加的头文件

inline void OutputLogHeader(const char *file, int line, const char *func, int level) {
  auto now = std::chrono::system_clock::now();
  auto now_ms = std::chrono::time_point_cast<std::chrono::milliseconds>(now);
  auto epoch = now_ms.time_since_epoch();
  auto value = std::chrono::duration_cast<std::chrono::milliseconds>(epoch);
  long duration = value.count();
  long seconds = std::chrono::duration_cast<std::chrono::seconds>(epoch).count();
  long milliseconds = duration - (seconds * 1000);

  tm *curTime = localtime(&seconds);
  char time_str[32];
  ::strftime(time_str, 32, "%S", curTime); /** 我们只需要秒以下的记录即可 */

  const char *type;
  switch (level) {
    case LOG_LEVEL_ERROR:
      type = "ERROR";
      break;
    case LOG_LEVEL_WARN:
      type = "WARN ";
      break;
    case LOG_LEVEL_INFO:
      type = "INFO ";
      break;
    case LOG_LEVEL_DEBUG:
      type = "DEBUG";
      break;
    case LOG_LEVEL_TRACE:
      type = "TRACE";
      break;
    default:
      type = "UNKWN";
  }
  
  ::fprintf(LOG_OUTPUT_STREAM, "%s.%03ld [%s:%d:%s] %s - ", time_str, milliseconds, file, line, func, type);
}


```


主要是怎么找到不一致



## 大锁 1th

bug:  `std::shared_ptr<T>::reset()`和**FrameHeader**提供的`Reset()`不要搞混了.



CheckedWritePage/CheckedReadPage的三个case

1. 请求的page已经在内存中, 直接拿来用就可以了;
2. 虽然内存中没有目标page, 但是有空闲的frame可以分配



1. Leaderboard.QPS.1 `./bin/bustub-bpm-bench --duration 30000 --latency 0`
2. Leaderboard.QPS.2 `./bin/bustub-bpm-bench --duration 30000 --latency 0 --lru-k-size 64 --db-size 640`
3. Leaderboard.QPS.3 `./bin/bustub-bpm-bench --duration 30000 --latency 1`

./bin/bustub-bpm-bench --duration 30000 --latency 0 > qps1.log & ./bin/bustub-bpm-bench --duration 30000 --latency 0 --lru-k-size 64 --db-size 640 > qps2.log & ./bin/bustub-bpm-bench --duration 30000 --latency 1 > qps3.log &

scan: 2501.1494252873563
get: 191.90404797601198

1. `WritePage`带bpm锁IO，`ReadPage`不带锁IO,  开启并行IO: **qps1~3均仅出现WritePage插队现象, 仅发现qps3出现一致性问题**

2. `WritePage`带bpm锁IO，`ReadPage`带读锁IO,  开启并行IO: **qps1~3均仅出现WritePage插队现象, 但不影响一致性, 符合猜测**

3. `WritePage`带写锁IO，`ReadPage`不带锁IO,  开启并行IO: **qps1~3均出现WritePage和ReadPage插队现象, 仅发现qps3出现一致性问题**

4. `WritePage`带写锁IO，`ReadPage`带读锁IO,  开启并行IO: **qps1~3均出现WritePage和ReadPage插队现象, 有一致性问题**


# 继续优化

现在的分数:

qps1:  scan: 39731.907612318355 get: 32477.682975603253
qps2:  scan: 42217.520847915206 get: 41258.6506016065
qps3:  scan: 16306.617270788913 get: 3956.852078891258

使用MyPromise后

qps1:  scan: 35219.907612318355 get: 28560.682975603253
qps2:  scan: 37305.520847915206 get: 36458.6506016065
qps3:  scan: 16570.617270788913 get: 3903.852078891258

cmake .. -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_CXX_COMPILER=/usr/bin/clang++-14

cmake .. -DCMAKE_BUILD_TYPE=RelWithDebInfo
make -j`nproc` bpm-bench
sudo perf record -g ./bin/bustub-bpm-bench --duration 30000 --latency 1
sudo perf script -i perf.data > profile.linux-perf.txt


查看火焰图

sudo perf record -g ./bin/bustub-bpm-bench --duration 30000 --latency 1
sudo perf script > out.perf
sudo ~/FlameGraph/stackcollapse-perf.pl out.perf > out.folded
~/FlameGraph/flamegraph.pl out.folded > kernel.svg

## readerwriterqueue 优化

如果采用标准库的`std::promise`和`std::future`,那么总占用的时间是:9.15+13.24

采用第一个版本`MyPromise`和`MyFuture`, 总占用:7.62+20.08

采用第二个版本`MyPromise`和`MyFuture`, 总占用:7.72+18.64

Channel::Get()  4.43
Channel::Put()  1.18

## lru-k优化

第一次尝试, 